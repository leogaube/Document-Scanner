\documentclass[bibliography=totoc]{scrartcl}
\usepackage[ngerman, english]{babel}
\usepackage{rwukoma}
\usepackage[pdfusetitle]{hyperref}
\usepackage{lipsum,caption}
\usepackage{acronym}
\usepackage{algorithm, algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}

\title{Document Scanner}
\author{Leopold Gaube and Florian Betz}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents

	\clearpage	

    \section{Algorithm Overview}
	Not everybody has a physical document scanner in their own household. 
	However, there are plenty of smartphone apps available that can transform any photograph of a document into an "scanned version" without any distracting background or perspective distortions.
	In the following report we will present our own approach for scanning a document in an image.
	We do not want to implement everything from scratch, so we will be using \ac{OpenCV}, an open source computer vision library that fits our purpose.
	It is available for C, C++, Python and Java. \cite{OpenCV}
	We will be using Python 3.8 and our entire code and test images can be found on \hyperlink{Gitlab}{https://gitlab.com/gaubeleo/document-scanner}.
	
	The algorithm consists of three main parts, each with its own challenges.
	First, we have to localize the document in the image. 
	Then we use a perspective transform from the corner points of the document to the entire span of the image in order to obtain a top-down view without perspective distortions.
	Finally, we use a binary filter to distinguish text from background and save the resulting image as our scanned document.

	However, we also want our algorithm to work under bad lighting conditions [Fig. 1b], taken from an extreme angle [Fig. 1c], with a document on a low contrast background [Fig. 1d].
	Of course the final image will suffer in quality compared to a document taken under near-optimal conditions, but our goal is to still obtain a readable document.
	
	\section{Document Localization}
	Some smartphone scanner apps require the user to mark the four corner points of the document manually. 
	However, we want to automate this process by detecting the corner points using Computer Vision.
	Detecting the corner points consistantly is probably the hardest and most crutial step of the entire algorithm. 
	
	The information content of any image is really high consisting of width x height x channels indiviual values, usually in the magnitude of millions of pixels.
	Our first goal is to find the document outline in the input image.
	For this step we don't need a high resolution, so we will reduce the information content by first downsizing the input image to something more manageable.
	In the process we will also get rid of noice which could potentially reduce the algorithms performance.
	We resize the image to 400 pixels on the larger side (width or height) and adjust the smaller side according to the original aspect ratio.
	
	In addition to downsizing the image, we also use a Gaussian Blur (with a 5x5 kernel) in order to smooth the image even further.
	Now we can use a Canny Edge Detector to find locally sudden changes in brightness or color. 
	The resulting binary image distinguishes between edges (depicted as white) and non-edges (black) for each pixel location in the original image.
	
	The Canny Edge Detector uses two thresholds for the hysteresis procedure. 
	The first threshold parameter define the minimum und the second parameter define the maximum bound.
	For all values above the maximum bound are sure to identified as corners. 
	Similarly for all value below the minimum bound are sure to identified as non corners. 
	If the value is between the lower bound and upper bound, it is rejected
	
	We are only concerned that the document outline shows up in our edge image, which is how we chose an upper threshold of 150 that works consistently even for documents on medium-contrast backgrounds. 
	However, we encountered a problem that on some images the document outline was missing a single pixel in the edge image and therefore resulting in an unclosed contour.
	That is why we chose a low value of 50 for the lower threshold that helps to increase the chances of obtaining a closed contour.
	
	From this point on, we have to assume that there exists a closed contour in our edge image which matches the outline of our document. 
	When trying to find the location of the document corners, it is ok if we miss them by a couple of pixels, but detecting the wrong contour would make the resulting document unreadable.

	Detecting contours is easy with OpenCV using the cv2.findContours() method.
	However, we have to distinguish the one corresponding to our document from all the other contours in the image.
	Other contours may include text, graphs and pictures on the document itself or even objects e.g. a stapler on the desk. 
	The contour we are looking for should have one of the largest perimeters and it should be able to be approximated it with only four points.
	Both of these assumptions will help us to find the right one, as it is highly unprobable another contour meets both criteria. 
	We will just pick the largest contour that can be approximated using four points as show by the following pseudo code:
	
	\section{Perspective Transformation}
	For the document localization part, we have been working with a low resolution image, making the text unreadable.
	Now we want to create a top-down view of the document with readable text, so we need to project the detected corner points back onto the original resolution. 

	When detecting contours, OpenCV does not care about the order of the points which may result in a rotated image when applying the perspective transformation.
	So we first have to write a function that sorts our corner points in a consistent manner.
	The following pseudo code algorithm \ref{alg:cap} can be used to sort points in the following order: top left, bottom left, top right, bottom right.


\begin{algorithm}[hbt!]
\caption{pseudo code corner sort}\label{alg:cap}
\begin{algorithmic}
\Require $corners$
%\Ensure $y = x^n$

%\State $x$ \gets sort(corners[:, 0, 0]) \Comment{sorted x-axis}
%\State $y$ \gets sort(corners[:, 0, 1]) \Comment{sorted y-axis}
\State $sortedcorners \gets []$

\While{$i \in x$}
\If{$i$ is equal to x[0]} or $i$ is equal to x[1]
    \State $sortedcorners \gets sortedcorners \cup corners[i]$
\EndIf
\EndWhile
\While{$i \in x$}
\If{$i$ is equal to x[2]} or $i$ is equal to x[1]
    \State $sortedcorners \gets sortedcorners \cup corners[i]$
    
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
	
	\section{Thresholding}
	For the last step of our project we convert the scanned document into a binary image, thus each pixel is supposed to be either black for text and graphs or white for the background.
	The easiest way to achieve this is by converting the color image into grayscale and setting a threshold value. 
	Any pixel value below this threshold would become black, pixel values above would be set to white.

	However, there is a problem with this simple thresholding approach, as it performs poorly under bad lighting conditions as can be seen in [Fig. 3a].
	When taking pictures of documents, a user may cast a shadow with their camera or mobile phone onto the document. 
	This can be problematic if a region in the shadows has lower pixel values for the background than the text in a well-lit region.
	In such a scenario, it would be impossible to find a static threshold that works well for the entire document.
	So instead we need an adaptive approach that looks at a set of neighboring pixels and chooses a threshold for this local region dynamically.

	The corresponding OpenCV function takes multiple parameters and after some experimenting we settled on a 11 x 11 neighbourhood and a constant C of 5. 
	Furthermore, we prefered the adaptive Gaussian method over the adaptive mean method, making the text look smoother.
	We determined these parameters empirically, depending on which values resulted in the most readable documents.

	It should be noted that an input image with a lot of pixel noise will result in a noisy scanned document. 
	The following image was taken with a camera using an ISO setting of 3200.
	In order to reduce the black artifacts we could smooth the image before the thresholding. 
	Unfortunately it would also compromise the text readability and is therefore inadvisable. 

	\section{Known Problems and Possible Solutions}
	Overall, our algorithm works fairly well and consistently even under difficult lighting conditions or when the picture of the document was taken from an unusual angle.
	However, we also observed some use cases where it fails:


	\begin{figure}[h!]
		\centering
		\begin{subfigure}[b]{0.3\linewidth}
			\includegraphics[width=\linewidth]{imgs/not_working/white_background.jpg}
			\caption{A white background results in low contrast around the document outline}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\linewidth}
			\includegraphics[width=\linewidth]{imgs/not_working/bent_corner.jpg}
			\caption{A document with a bent corner is hard to approximate with a four point contour}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\linewidth}
			\includegraphics[width=\linewidth]{imgs/not_working/missing_corner.jpg}
			\caption{One of the document corners is cut off from the image composition}
		\end{subfigure}
		\caption{These are example images on which our algorithm (currently) does not work on.}
		\label{fig:known_problems}
	  \end{figure}
	
	If the document is placed on a low-contrast background (e.g. a white desk), our algorithm has problems detecting an edge where the document outline should be.
	In some cases it may already help to do more preprocessing e.g. contrast enhancement.
	Currently, our program uses the Canny Edge Detector only once with a fixed lower and upper threshold, because this works fine for the majority of images.
	Whenever the document localization fails, we could try to repeat the process with lower thresholds in order to increase our chance of success. 
	This will introduce more edge artifacts in the edge image, but it might also be enough to detect the document outline.


	Another situation in which our program works poorly is with a bent document corner. 
	This may break the localization algorithm, because the detected contour would - theoretically - have to be approximated using five instead of four corner points.
	In some cases with minor bent corners this still works ok, as we allow for an error of 3\% when approximating the document contour(see Fig. 4a) which may still result in a four point contour approximation. 
	The resulting image will definitely have some small distortion due to the inaccurate localization of the bent corner, but at least the algorithm does not fail entirely.
	The same problem arises if the user is not careful while taking the image and accidentally cutting off a single corner from the picture composition.
	
	For the last two problematic use cases, we already have a possible solution in mind: 
	Whenever the document localization fails, we could try a completely different approach using a Hough Transformation. 
	Hough Transformations are very good at detecting straight lines in edge images. 
	Detecting the four most prevalent lines (with maximum-supression) in an edge image, corresponds to detecting the most likely document edges.
	The intersections of any two lines give us the location of each document corner. 
	A bent/hidden corner or even a corner outside of the image composition should be less of a problem when using the Hough Transformation approach.
	Nevertheless, it has its own downside, because the most prevalent lines have to be document edges and not for instance the edge of a desk.
	Therefore, the Hough Transformation would not replace our approach using contours, but rather act as a backup plan whenever our first document localization fails.
	
	\begin{figure}[h!]
		\centering
		\begin{subfigure}[b]{0.3\linewidth}
			\includegraphics[width=\linewidth]{imgs/not_working/bent_corner_hough.jpg}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[width=\linewidth]{imgs/not_working/missing_corner_hough.jpg}
		\end{subfigure}
		\caption{A Hough Transformation for detecting the four most prevalent lines (in blue). Using the intersections of these lines could potentially fix problems with missing corners. Both figures were created manually and are for illustrative purposes only.}
		\label{fig:missing_corner_hough}
	\end{figure}


\section*{List of Acronyms} 
\addcontentsline{toc}{section}{List of Acronyms}

\begin{acronym}[....]
    \acro{OpenCV}{Open Source Computer Vision Library}
\end{acronym}
\bibliographystyle{alpha}			
\bibliography{literature}
\end{document}
